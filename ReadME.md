# KnitSpace: Automated LLM Testing Harness

KnitSpace is an automated testing harness designed to evaluate and compare the capabilities of various Large Language Models (LLMs) across a diverse set of tasks. It provides a comprehensive framework for researchers and developers to assess LLM performance in areas such as problem-solving, knowledge retrieval, coding proficiency, and safety.

## Key Features

*   **Multi-LLM Provider Support**: Easily integrate and test models from various providers including OpenAI, Google (Gemini), Cohere, Mistral, Groq, Together, Cerebras, OpenRouter, NVIDIA, and Cloudflare.
*   **Diverse Test Suite**: Evaluate models on a wide array of tasks such as:
    *   Mathematical reasoning
    *   Coding challenges (Python, JavaScript)
    *   Instruction following and rule adherence
    *   Knowledge-based question answering (e.g., MMLU, MedMCQA)
    *   Long-context understanding
    *   Text obfuscation and de-obfuscation
    *   Ethical and safety evaluations
*   **Sophisticated Elo Rating System**: Go beyond simple accuracy metrics with a configurable Elo rating system that factors in task difficulty and an "action cost" (derived from a GPT-2 model) to assess the quality and efficiency of responses.
*   **Secure Code Execution Environment**: Safely run and validate LLM-generated code for coding tests using Docker containers, preventing unintended side effects on the host system.
*   **Text Obfuscation Capabilities**: Test advanced reasoning by transforming questions using techniques like character mapping, requiring LLMs to understand and apply provided de-obfuscation rules.
*   **Interactive Web-Based Review**: Visually inspect and analyze test results through a local web server, examining individual question-answer pairs, verification status, and scoring metrics.
*   **Extensible Framework**: Designed for easy expansion, allowing for the addition of new LLM providers, test types, and custom evaluation metrics.

## Core Components

KnitSpace is organized into several core components that work together to provide a flexible and comprehensive LLM testing environment:

*   **`knit_space/models.py`**: This module is the heart of LLM interaction.
    *   It defines an abstract base class `Model` ensuring a consistent interface for all LLM providers.
    *   It contains specific implementations (e.g., `OpenAIModel`, `GeminiModel`) for various providers, handling API authentication, request formatting, and response parsing.
    *   The `ProviderInterface` class wraps these specific model implementations, offering a unified way to list available models from a provider and perform inference.
    *   The `MultiModelInterface` class manages multiple `ProviderInterface` instances, allowing for easy configuration and use of different LLMs from different providers simultaneously.

*   **`knit_space/tests/`**: This directory houses the diverse suite of evaluation tasks.
    *   `base.py` defines foundational classes:
        *   `QAItem`: A dataclass representing a single question-answer item, including its text, expected answer, skill coefficient (for difficulty), modality, and verification logic.
        *   `AbstractQATest`: An abstract base class for all test generators. Subclasses implement `generate()` to yield `QAItem` instances.
        *   `TestRegistry`: Facilitates the discovery and organization of test classes, allowing them to be tagged and retrieved.
    *   Various `*Test.py` files (e.g., `basic_math_tests.py`, `coding_tests.py`, `mmlu_obfuscated_tests.py`) implement specific test logic, generating `QAItem`s for different evaluation scenarios.

*   **`knit_space/marker.py`**: This module is responsible for scoring and evaluating the LLM's performance.
    *   It collects results from executed tests (the `QAItem`, the LLM's generated answer, and the verification outcome).
    *   It calculates summary statistics (e.g., number of attempted, correct, and failed questions).
    *   A key feature is its sophisticated **Elo rating system**. This system goes beyond simple accuracy by:
        *   Considering a `skill_coefficient` associated with each `QAItem`.
        *   Calculating an "action trajectory S-value" using a pre-trained GPT-2 model. This value represents a form of "cognitive cost" or "complexity" for generating an answer.
        *   Combining these factors to produce a more nuanced Elo score reflecting both correctness and the nature of the answer.
    *   It includes a Flask-based web server that can be launched (`launch_review_server()`) to interactively review detailed test results.

*   **`knit_space/utils/code_executor.py`**: This utility provides a secure environment for executing code generated by LLMs, which is crucial for coding tests.
    *   It uses Docker containers to sandbox code execution, currently supporting Python and JavaScript.
    *   It takes the LLM-generated code, a target language, a set of test cases (inputs and expected outputs), and problem hints (like function names).
    *   It dynamically prepares runner scripts, executes them within Docker, and returns detailed results, including pass/fail status, actual output, and any errors.

*   **`knit_space/obscurers/`**: This directory contains tools for transforming text, primarily used to create more challenging test scenarios.
    *   `char_obfuscator.py` provides `CharObfuscator`, which can replace characters in a text with symbols from a defined set (e.g., mapping English letters to Greek letters).
    *   This is used in tests like `MMLUObfuscatedQATest` to evaluate an LLM's ability to understand and apply a given obfuscation map.

## Setup

Follow these steps to set up the KnitSpace environment:

### 1. Prerequisites

*   **Python**: Python 3.8 or newer is recommended.
*   **Docker**: Required if you intend to run coding tests that utilize the `CodeExecutor`. Ensure Docker Desktop is installed and the Docker daemon is running.
*   **Git**: For cloning the repository.

### 2. Installation

1.  **Clone the repository**:
    ```bash
    git clone <repository_url> # Replace <repository_url> with the actual URL
    cd knitspace # Or your repository's directory name
    ```

2.  **Create a virtual environment** (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies**:
    KnitSpace relies on several external libraries. Install them using pip:
    ```bash
    pip install requests numpy torch transformers Flask Pillow
    ```
    You will also need to install the Python SDKs for the specific LLM providers you intend to use. Here are some common ones:
    ```bash
    pip install google-generativeai openai mistralai cohere-sdk groq together cerebras-cloud-sdk nvidia-ml-py3 # For NVIDIA API
    ```
    *Note: For `cerebras-cloud-sdk`, ensure you have the necessary credentials and configuration as per their documentation.*
    *A `requirements.txt` file will be added in a future update for easier dependency management.*

### 3. Configuration

*   **API Keys**: Most LLM providers require API keys for authentication. KnitSpace expects these keys to be set as environment variables. The specific variable names are typically:
    *   `OPENAI_API_KEY` for OpenAI models
    *   `GEMINI_API_KEY` (or `GOOGLE_API_KEY`) for Gemini/Google models
    *   `MISTRAL_API_KEY` for Mistral models
    *   `COHERE_API_KEY` for Cohere models
    *   `GROQ_API_KEY` for Groq models
    *   `TOGETHER_API_KEY` for Together AI models
    *   `CEREBRAS_API_KEY` for Cerebras models
    *   `NVIDIA_API_KEY` for NVIDIA API Catalog models
    *   `NVIDIA_NIM_BASE_URL` for NVIDIA NIM (set this as the API key if using NIM)
    *   `OPENROUTER_API_KEY` for OpenRouter models
    *   `CLOUDFLARE_API_KEY` for Cloudflare Workers AI

    Set these variables in your shell environment or `.bashrc`/`.zshrc` file. For example:
    ```bash
    export OPENAI_API_KEY="your_openai_api_key_here"
    ```

*   **Cloudflare Specific**: If using Cloudflare models, you must also set the `CLOUDFLARE_ACCOUNT_ID` environment variable:
    ```bash
    export CLOUDFLARE_ACCOUNT_ID="your_cloudflare_account_id"
    ```

*   **Docker (for CodeExecutor)**: Ensure Docker is running. No specific configuration within KnitSpace is usually needed if Docker is installed and accessible from your terminal.

## Running Tests

The primary script for running comprehensive evaluations is `verify-auto.py`.

### 1. Using `verify-auto.py`

`verify-auto.py` orchestrates the testing process: it loads specified test cases, runs them against a configured LLM, uses the `Marker` to evaluate results, calculates an Elo score, and launches a web server for detailed review.

**Steps to run:**

1.  **Ensure Setup is Complete**: Verify that you have installed all necessary dependencies and configured the required API keys as environment variables (see Setup section).
2.  **Configure `verify-auto.py`**:
    *   **Select LLM Provider and Model**: Open `verify-auto.py` in a text editor.
        *   Modify the `models.get_provider(...)` line to specify the provider you want to use (e.g., `"openai"`, `"google"`, `"mistral"`).
        *   Update the `model_name` parameter in the `inference()` call (e.g., `"gpt-4o"`, `"models/gemini-1.5-pro-latest"`, `"mistral-large-latest"`). Refer to `knit_space/models.py` or the provider's documentation for available model IDs.
        *   Ensure the corresponding API key environment variable for your chosen provider is set.
        ```python
        # Example: Changing to OpenAI's GPT-4o in verify-auto.py
        # os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here" # Ensure this is set in your environment
        # llm_provider = models.get_provider("openai") 
        # ...
        # generated_answer = llm_provider.inference(
        # model_name="gpt-4o", # Changed model
        # prompt=query.question,
        # ...
        # )
        ```
    *   **Select Test Cases**: In the same script, edit the `test_cases` list to include the test classes you want to run. Test classes are imported from `knit_space.tests`.
        ```python
        # Example: Selecting specific tests in verify-auto.py
        # test_cases = [
        #     tests.MathQATestClass,       # For math problems
        #     tests.CodingQATestClass,     # For coding problems
        #     tests.MMLUObfuscatedQATest,  # For MMLU with obfuscation
        #     # Add or remove other tests as needed
        # ]
        ```
3.  **Run the script**:
    ```bash
    python verify-auto.py
    ```
4.  **View Results**:
    *   The script will output logs to the console, including the progress of tests.
    *   Upon completion, the final Elo score will be printed.
    *   A Flask web server will automatically launch (typically on `http://localhost:8000` or `http://127.0.0.1:8000`). Open this URL in your browser to review the detailed results of each test item.

    *(Note: The `time.sleep(10)` in `verify-auto.py` is a precaution for API rate limits. Adjust or remove it based on your needs and the provider's policies.)*

### 2. Inspecting Test Data with `QA-test.py`

If you want to generate and view the content of test questions and their expected answers *without* running them against an LLM, you can use `QA-test.py`.

1.  **Configure `QA-test.py`**: Edit the `test` list in `QA-test.py` to include the test classes you want to inspect.
2.  **Run the script**:
    ```bash
    python QA-test.py
    ```
    This will print the generated questions and answers to the console.

### Future Enhancements (Suggested)

For more flexible execution, future versions could incorporate:

*   Command-line arguments to `verify-auto.py` for selecting LLM providers, models, and specific tests or tags.
*   Configuration files (e.g., YAML or JSON) to define test runs and model parameters.

## Explaining Results

After running an evaluation using `verify-auto.py`, you can analyze the LLM's performance through two main channels:

### 1. Console Output

*   **Progress Bar**: During execution, `tqdm` will display a progress bar showing the number of test items processed.
*   **Final Elo Score**: Once all tests are completed, a final Elo score will be printed to the console. For example:
    ```
    Final Elo Score: 1053.281762
    ```
    The Elo score is a comparative measure of performance. Higher scores generally indicate better performance. The scale and interpretation can depend on the specific set of tests run and the `Marker` configuration (e.g., `action_base_flops`, `action_alpha`). It's most useful when comparing different models run against the same test suite and configuration.

### 2. Web Review Server

The `Marker` component automatically launches a Flask-based web server at the end of the `verify-auto.py` run. This server provides an interactive interface to drill down into individual test results.

*   **Accessing the Server**: Open your web browser and navigate to the URL displayed in the console (typically `http://localhost:8000` or `http://127.0.0.1:8000`).
*   **Information Displayed**:
    *   **Summary Statistics**: The main page shows overall statistics:
        *   Total Attempted items
        *   Correctly Solved items
        *   Failed items
        *   The final calculated Elo Score.
    *   **Individual Item Details**: For each test item, the review server presents:
        *   **Item ID and Status**: A unique identifier for the test item and whether it was a `PASS` or `FAIL` based on the `QAItem.verify()` method.
        *   **Skill Coefficient**: The difficulty weighting assigned to the question.
        *   **Trajectory S, Cost (log10S), InvAction (1/Cost)**: These are metrics related to the Elo calculation.
            *   `Trajectory S`: The "action value" computed by the auxiliary GPT-2 model in the `Marker`. Lower S-values (for correct answers) generally contribute more to the Elo score, representing more "efficient" or "insightful" answers.
            *   `Cost (log10S)`: The logarithm of the S-value, used in the Elo formula.
            *   `InvAction (1/Cost)`: The component directly added to the Elo score for that item (if correct).
        *   **Question**: The full text of the question presented to the LLM (this may include obfuscation maps if applicable).
        *   **Expected Info (qa_item.answer)**: The ground truth answer or information expected. For coding tests, this might be a description or not directly comparable if verification relies on execution.
        *   **Generated Answer**: The raw response from the LLM.

This detailed breakdown allows for qualitative analysis of an LLM's successes and failures, going beyond aggregate scores.

## Extending the Harness

KnitSpace is designed to be extensible. You can add support for new LLM providers or create entirely new types of tests.

### 1. Adding New LLM Providers

To integrate an LLM provider not currently supported:

1.  **Create a New Model Class**:
    *   In `knit_space/models.py`, create a new class that inherits from `knit_space.models.Model`.
    *   Implement the required abstract methods:
        *   `_initialize_client(self)`: Initialize the provider's API client. Store it in `self._client`. Handle cases where the SDK might not be installed.
        *   `inference(self, prompt: Any, **kwargs) -> Any`: Send the prompt to the provider's API and return the LLM's response. Adapt the prompt format if necessary and parse the response structure of the provider.
        *   `_list_api_models(self) -> List[Dict[str, Any]]`: (Optional but recommended) Implement logic to query the provider's API for a list of available models. This is used by `ProviderInterface.list_models()`. If not feasible, you can return a static list or rely on `KNOWN_MODELS_INFO`.
2.  **Update Provider Mappings**:
    *   Add your new model class to the `PROVIDER_CLASS_MAP` dictionary in `knit_space/models.py`. The key should be a lowercase string identifying the provider (e.g., `"newprovider"`).
    *   Add an entry to the `KNOWN_MODELS_INFO` dictionary if you have specific model IDs, user-friendly names, modalities, or default parameters you want to pre-define.
    *   Update the `_get_api_key_for_provider()` helper function in `knit_space/models.py` to include the environment variable name for your new provider's API key.
    *   If your provider requires additional configuration parameters (like Cloudflare's `account_id`), update `_get_additional_config_for_provider()`.

### 2. Adding New Tests

To create new types of evaluation tasks:

1.  **Create a New Test Class**:
    *   In the `knit_space/tests/` directory (or a subdirectory), create a new Python file for your test (e.g., `my_new_test.py`).
    *   Define a class that inherits from `knit_space.tests.base.AbstractQATest`.
    *   Implement the `generate(self, count: int = 1, **kwargs) -> Iterator[QAItem]` method:
        *   This method should yield instances of `knit_space.tests.base.QAItem`.
        *   For each `QAItem`, you need to define:
            *   `id`: A unique string identifier.
            *   `question`: The question prompt to be sent to the LLM. This can be a string or a more complex structure if your test requires it (e.g., a dictionary for multi-part prompts).
            *   `answer`: The ground truth answer.
            *   `skill_coefficient`: A float indicating the relative difficulty or importance of the question (used in Elo calculation).
            *   `modality`: A string indicating the primary modality (e.g., `"text"`, `"code"`, `"image"`).
            *   `metadata` (optional): A dictionary for any additional information.
            *   `verification_fn` (optional): A custom callable `(expected_answer, provided_answer, qa_item_instance) -> bool`. If not provided, verification defaults to exact match (`expected_answer == provided_answer`).
                *   For coding tests, this function would typically use the `CodeExecutor` from `knit_space.utils.code_executor` to run the LLM's code against test cases.
                *   For tests with complex verification logic, this is the place to implement it.
    *   Override `supported_modalities` property if your test supports modalities other than just `['text']`.
    *   Optionally, implement `setup_resources()` and `cleanup_resources()` if your test generator needs to manage external resources.
2.  **Register Your Test (Optional)**:
    *   You can make your test discoverable by the `TestRegistry` by using the `@register_test("tag1", "tag2")` decorator above your class definition.
3.  **Use Your Test**:
    *   Import your new test class in `verify-auto.py` (or other execution scripts) and add it to the `test_cases` list.

By following these patterns, you can seamlessly integrate new functionalities and broaden the evaluation scope of the KnitSpace harness.
